---
title: "EML Template R Notebook"
output: html_notebook
---

This is an R notebook that serves as a template for converting data to EML format using EMLassemblyline. To create another code box, press option + command + i (this shouldn't be needed).   

Note: This template allows users to edit tables directly in RStudio, but you might find that it is easier to edit some of these documents in Excel. Any files in tabular format can be opened in Excel by either dragging the document into Excel or right-clicking on the document and selecting "Open with".   

**Avoid copying text from Word documents, as this will likely cause UTF-8 errors later on**


Step 1: Set Working Directory 
```{r}
#What is the working directory
getwd()

# Create directory specificly for project
#dir.create("PROPHET_Aerosols_Ozone_2014")

# Set working directory (uncomment and edit for YOUR own folder)
setwd ("/Users/your_name/Desktop/your_folder")
```


Step 2: Install and Load Packages
```{r}
# Clear the working space
rm(list = ls())

# Install needed packages for data cleaning
#install.packages("tidyverse")
#install.packages("devtools")
install.packages("editData")
#install.packages("EML") #if your EML package version is too old for EMLassemblyline, run this to update

# Load the packages used in this R Notebook
library(tidyverse)
library(devtools)

# Install and load EMLassemblyline
install_github("EDIorg/EMLassemblyline")
library(EMLassemblyline)
```

Step 3: Identify Data and Data Cleaning
```{r}
#run any cleaning processes necessary on your data sets
#make sure your data sets (as csv files) are the only contents in your working directory
```


**Now would be a good time to go through the template and replace all "Master_AGB_data_for_UMBS_IMS_tree_list_tidy" etc. with the names of your own data files (add or delete as necessary). The easiest way to do this is to do a search-and-replace, by pressing command + 'f'. Put "Master_AGB_data_for_UMBS_IMS_tree_list_tidy" in the "find" section and the desired file name in the "replace" section, and then click the "replace" button until complete**  

Step 4: Import templates from EMLassemblyline

* This step creates template files for you to fill out to simplify the EML writing process. You will fill in these files in the following steps
* Create a new directory for your dataset. This is where the metadata parts created in the assembly line process will be stored and available for editing should you need to change the content of your EML. Name this directory after your dataset. Replace spaces with underscores (ex: "your directory" should be "your_directory")
* Rename these files following these rules:
    + replace symbols with words
    + replace parentheses with underscores
    + replace periods with underscores
    + replace blank spaces with underscores
* Set your intellectual rights:
    + CC0 : This data package is released to the “public domain” under Creative Commons CC0 1.0 “No Rights Reserved”  
    + CCBY : This information is released under the Creative Commons license - Attribution - CC BY and requires attribution  
    
    
    
NOTE TO SELF: IMPORT_TEMPLATES DOESN'T LIKE HAVING "#" IN THE PLOT COLUMN VALUES, READS AS A DELIMITER. I REPLACED WITH "_" AND THAT WORKED FINE... NOT IDEAL
```{r}
# View documentation for fuctions
?import_templates

# Import templates for specific dataset
# Note: data.files extensions (such as .csv) not required
import_templates(path = "G:/My Drive/UMBS Fellowship/UMBS data sets in progress/LTREB/LTREB eml/", 
                 license = "CCBY", #CC0 or CCBY
                 data.files = c("Master_AGB_data_for_UMBS_IMS_tree_list_tidy")) 
```


Step 5: Edit Abstract  

* The abstract should cover what, why, when, where, and how for your dataset. Write your abstract in plain text.
* Do not use special characters, symbols, formatting, or hyperlinks (URLs are acceptable). The reason for this is that the EML schema only allows characters that are apart of the unicode character set  
* Replace any non-unicode characters if copying text using this website (for UTF-8 errors): https://pteo.paranoiaworks.mobi/diacriticsremover/  
* A difference-checker such as https://www.diffchecker.com may be useful in finding bad characters after non-unicode characters are removed 
```{r}
# Open and Edit Abstract text file
file.edit("abstract.txt")
```

Step 6: Edit Methods

* Describe the methods for your dataset. Be specific, include instrument descriptions, or point to a protocol online. If this dataset is a synthesis of other datasets please specify dataset origins, preferably their DOI or URL plus general citation information  
* Do not use special characters, symbols, formatting, or hyperlinks (URLs are acceptable). The reason for this is that the EML schema only allows characters that are apart of the unicode character set
* Replace any non-unicode characters if copying text using this website (for UTF-8 errors): https://pteo.paranoiaworks.mobi/diacriticsremover/  
* A difference-checker such as https://www.diffchecker.com may be useful in finding bad characters after non-unicode characters are removed 
```{r}
# Open and Edit Methods text file
file.edit("methods.txt")
```

Step 7: Edit Additional Information

* This is a good place for text based information about your dataset that doesn't fall under the scope of the abstract or methods (e.g. a list of research articles or theses derived from this dataset)
* Do not use special characters, symbols, formatting, or hyperlinks (URLs are acceptable). The reason for this is that the EML schema only allows characters that are apart of the unicode character set
* Replace any non-unicode characters if copying text using this website (for UTF-8 errors): https://pteo.paranoiaworks.mobi/diacriticsremover/  
* A difference-checker such as https://www.diffchecker.com may be useful in finding bad characters after non-unicode characters are removed 
```{r}
# Open and Edit Additional Information text file
file.edit("additional_info.txt")
```

Step 8: Edit Keywords  

* List the keywords that best describe your dataset
* Consult the LTER controlled vocabulary (http://vocab.lternet.edu/vocab/vocab/index.php) for keywords
* Include keywords that describe your data in addtion to keywords that describe your lab, station, and project (e.g. OBFS, LTREB, etc.)  
* Definitions for columns of this file:
    + keyword : A keyword describing your dataset.
    + keywordThesaurus : A keywordThesaurus (i.e. a controlled vocabulary like the resource listed above) corresponding to the keyword listed in the keyword column. If the keyword is not from a thesaurus or controlled vocabulary, leave corresponding entry in the keywordThesaurus column blank.
```{r}
# Read keywords table into data frame
keywords <- read_tsv("keywords.txt") 

# Edit dataframe keywords
#newkeywords <- editData::editData(keywords) 
newkeywords <- edit(keywords)

# Output to new keywords.txt file
write_tsv(newkeywords, path = "keywords.txt")
```

Step 9: Edit Personnel

* Definitions for columns of this file:  
    + givenName : First name of person.  
    + middleInitial : Middle initial of person.  
    + surName : Last name of person.  
    + organization : Name Name of organization the person is associated with.
    + electronic : MailAddress Email address of person.  
    + userId : ORCID of person (not required). A valid entry for userId is the 16 digit ORCID number separated by dashes (i.e. XXXX-XXXX-XXXX-XXXX). An ORCID is like a social security number for scientists and links your dataset with your ORCID. Create one here (https://orcid.org)  
    + role : Role of person with respect to this dataset. Valid entries for role are:  
       - creator : Dataset creator (required; at least 1 creator must be listed for your dataset).  
       - PI : Principal investigator associated with this dataset (not required).  
        - contact : Dataset contact (required; at least 1 contact must be listed for your dataset). The contact may be a person or a position at an organization. We recommend listing the contact as a person rather than a position. To list a position as a contact (e.g. Data Manager), Enter the position name in the givenName column and leave middleInitial and surName blank.  
        - Any other entries into the 'role' column are acceptable and will be defined under the associated party element of this dataset with whatever value is entered under role.  
        - If a person serves more than one role, duplicate this persons information in another row but with the additional role.  
        - Similarly if a role is shared among many people, list the individuals with the shared role on separate lines.  
    + projectTitle : Title of the project this dataset was created under (optional). Project titles are only listed on lines where the personnel role is PI. If an auxiliary project was involved in creating this dataset then add a new row below the row containing the primary project and list the project title and associated PI. Do this for each auxiliary project.   
    + fundingAgency : Name of the entity funding the creation of this dataset (optional). Only include an entry in this column for rows where role PI.  
    + fundingNumber : Number of the grant or award that supported creation of this dataset (optional). Only include an entry in this column for rows where role PI.  
```{r}
# Read Personnel into data frame
personnel <- read_tsv("personnel.txt")

# Edit Personnel dataframe
newPersonnel <- editData::editData(personnel)

write_tsv(newPersonnel, path = "personnel.txt")
```

Step 10: Edit Attributes

* An attributes_datatablename.txt file has been created for each of your data tables. You will see this file has been partially populated with information detected by the import_templates function. You will have to double check values listed in all the columns except attributeName.  
* Instructions for completing the attribute table are as follows:
    + attributeName : Enter attribute names (i.e. column names) as they appear in the data table and in the same order as listed in the data table.  
    + attribute : Definition Enter definitions for each attribute. Be specific, it can be lengthy.  
    + class : Enter the attribute class. This is the type of value stored under the attribute. Valid options for class are:    
        - numeric : For numeric variables.  
        - categorical : For categorical variables.  
        - character : For variables containing text or symbols that are not categorical.  
        - Date : For date time variables.  
        - If an attribute has class of numeric or Date, then all values of this attribute must be either numeric or date time. If any character strings are present in an otherwise numeric attribute, this attribute must be classified as character. Similarly if any values of a "Date" attribute do not match the date time format string (details below), then this attribute must be classified as character.  
    + unit : If an attributes class is numeric, then you must provide units. If the attribute is numeric but does not have units, enter dimensionless. If the attribute class is categorical, character, or Date then leave the unit field blank. If the attribute is numeric and has units search the standard unit dictionary for the unit of interest and enter the unit name as it appears in the dictionary (unit names are case sensitive).  
```{r}
# Read specific attributes file into data frame - replicate for all files

attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy <- read_tsv(file.choose()) # rename data frame and select corresponding file


# Look up needed info before populating
view_unit_dictionary()

# Edit attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy dataframe (two editor options: editData has a nice GUI, but have to save and exit to do anthing else in RStudio, edit function allows you to look up unit dictionary or help )
attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited <- editData::editData(attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy)  # or attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited <- edit(attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy)


# Write attributes edited in data frame to file
write_tsv(attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited, path = "attributes_Master_AGB_data_for_UMBS_IMS_tree_list_tidy.txt")


```

Step 11: Edit Custom Units  

* If you cannot find a unit in the dictionary, create one and add it to custom_units.txt
* If you have no custom units to report you may delete this file, but may also keep it around if you think it may be of future use. Valid custom units must be convertible to SI Units (i.e. International System of Units). If it cannot be converted to SI then list it in the attribute defintion and enter "dimensionless" in the unit field. To create a custom unit define the:  
    + id : This is equivalent to the unit name.  
    + unitType : The type of unit being defined. Reference the dictionary for examples.  
    + parentSI : The SI equivalent of the id you have entered.  
    + multiplierToSI : This is the multiplier to convert from your custom unit to the SI unit equivalent.  
    + description : A description of the custom unit. Reference the dictionary for examples.   
* dateTimeFormatString : Enter the date time format string for each attribute of "Date" class. Remember, a class of "Date" specifies the attribute as a date, time, or datetime. Enter the format string in this field. If the attribute class is not "Date", leave this field blank. Below are rules for constructing format strings.    
    + year : Use Y to denote a year string (e.g. 2017 is represented as YYYY).  
    + month : Use M to denote a month string (e.g. 2017-05 is represented as YYYY-MM).  
    + day : Use D to denote a day string (e.g. 2017-05-09 is represented as YYYY-MM-DD).  
    + hour : Use h to denote a hour string (e.g. 2017-05-09 13 is represented as YYYY-MM-DD hh).  
    + minute : Use m to denote a minute string (e.g. 2017-05-09 13:15 is represented as YYYY-MM-DD hh:mm).  
    + second : Use s to denote a second string (e.g. 2017-05-09 13:15:00 is represented as YYYY-MM-DD hh:mm:ss).
    + Time zone format strings : Use + or - along with a time string to denote time zone offsets (e.g. 2017-05-09 13:15:00+05:00 is represented as YYYY-MM-DD hh:mm:ss+hh:mm).  
* missingValueCode : If a code for 'no data' is used, specify it here (e.g. NA, -99999, etc.). Only one missingValueCode is allowed for a single attribute.
* missingValueCodeExplanation : Define the missing value code here.
```{r}
# Read Custom Units into data frame
custom_units <- read_tsv("custom_units.txt")

# Edit Custom Units dataframe
newCustom_units <- editData::editData(custom_units)

write_tsv(newCustom_units, path = "custom_units.txt")
```

**Make sure all files in dataset directory are closed! Some functions will error out if these files are open**

Step 11: Define Categorical Variables

* If your data tables contain any attributes with the categorical class, you will need to supply definitions for the categorical codes. The function define_catvars searches through each attribute file looking for attributes with a categorical class. If found, the function extracts unique categorical codes for each attribute and writes them to a file for you to define.
```{r}
# View documentation for this function
?define_catvars

# Run this function for your dataset
define_catvars(path = "G:/My Drive/UMBS Fellowship/UMBS data sets in progress/LTREB/LTREB eml/")
```
```{r}
# Read specific attributes file into data frame - replicate for all files

catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy <- read_tsv(file.choose()) # rename data frame and select corresponding file

# Look up needed info before populating
view_unit_dictionary()

# Edit catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy dataframe (two editor options: editData has a nice GUI, but have to save and exit to do anthing else in RStudio, edit function allows you to look up unit dictionary or help )
catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited <- editData::editData(catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy)  # or catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited <- edit(catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy)


# Write attributes edited in data frame to file
write_tsv(catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy_edited, path = "catvars_Master_AGB_data_for_UMBS_IMS_tree_list_tidy.txt")


```


Step 12: Extract Geographic Coverage 

* If your dataset contains more than one sampling location, then you will want to add this information to your metadata. Often a data user will search for data withing a geographic area. As of now the assembly line only supports point locations, multiple areas are not yet supported.  
* Run the function extract_geocoverage to get the unique latitude, longitude, and site name combinations from your data and write to file. extract_geocoverage requires specific inputs that may require altering the latitude and longitude formate of your data. See documenation for details.  
* Arguments required by this function are:  
    + path : A path to the dataset working directory containing the data table with geographic information.  
    + data.file : Name of the input data table containing geographic coverage data.  
    + lat.col : Name of latitude column. Values of this column must be in decimal degrees. Latitudes south of the equator must be prefixed with a minus sign (i.e. dash, "-").  
    + lon.col : Name of longitude column. Values of this column must be in decimal degrees. Longitudes west of the prime meridian must be prefixed with a minus sign (i.e. dash, "-").  
    + site.col : Name of site column. This column lists site specific names to be associated with the geographic coordinates.  
* This function outputs a tab delimited file named geographic_coverage.txt to your dataset directory. You may edit this if you'd like, but if the data table this information has been extracted from is accurate, then there is no need for editing.
```{r}
# View documentation for this function
?extract_geocoverage

# Run this function for your dataset
extract_geocoverage(path = "",
                    data.file = "",
                    lat.col = "",     
                    lon.col = "",
                    site.col = "")
```

Step 13: Make EML

* Now you are ready to synthesize your completed metadata templates into EML. This step is relatively simple, but requires several arguments:  
    + path : A path to the dataset working directory.  
    + dataset.title : A character string specifying the title for your dataset. Be descriptive (more than 5 words). We recommend the following format: Project name: Broad description: Time span (e.g. "GLEON: Long term lake chloride concentrations from North America and Europe: 1940-2016").  
    + data.files : A list of character strings specifying the names of the data files of your dataset.  
    + data.files.description : A list of character strings briefly describing the data files listed in the data.files argument and in the same order as listed in the data.files argument.  
    + data.files.quote.character : A list of character strings defining the quote characters used in your data files and in the same order as listed in the data.files argument. If the quote character is a quotation, then enter "\"". If the quote character is an apostrophe, then enter "\'". If there is no quote character then don't use this argument when running make_eml.     + zip.dir.description : At the moment make_eml() can detect a single .zip directory automatically (good for auxilliary files). This description will be a character string briefly describing the contents of the .zip directory.
    + data.files.url : A character string specifying the URL of where your data tables are stored on a publicly accessible server (i.e. does not require user ID or password). The EDI data repository software, PASTA+, will use this to upload your data into the repository. If you will be manually uploading your data tables, then don't use this argument when running make_eml.  
    + temporal.coverage : A list of character strings specifying the beginning and ending dates of your dataset. Use the format YYYY-MM-DD.  
    + geographic.description : A character string describing the geographic coverage of your dataset.  
    + geographic.coordinates : A list of character strings specifying the spatial bounding coordinates of your dataset in decimal degrees. The list must follow this order: "North", "East", "South", "West". Longitudes west of the prime meridian and latitudes south of the equator are prefixed with a minus sign (i.e. dash -). If you don't have an area, but rather a point. Repeat the latitude value for North and South, and repeat the longitude value for East and West.  
    + maintenance.description : A character string specifying whether data collection for this dataset is "ongoing" or "completed".  
    + user.id : A character string specifying your EDI data repository user ID. If you don't have one, contact EDI (info@environmentaldatainitiative.org) to get one, or don't use this argument when running make_eml.  
    + package.id : A character string specifying the package ID for your data package. If you don't have a package ID, then don't use this argument when running make_eml. A non-input package ID defaults to "edi.101.1".  
```{r}
# View documentation for this function
?make_eml

# Run the function
make_eml(path = "G:/My Drive/UMBS Fellowship/UMBS data sets in progress/LTREB/LTREB eml/",
         dataset.title = "LTREB Above Ground Biomass at the University of Michigan Biological Station, Pellston, Michigan from 1934 to 2014",
         data.files = c("Master_AGB_data_for_UMBS_IMS_tree_list_tidy.csv"),
         data.files.description = c("Above ground biomass measurements for individual trees in the UMBS plots"),
         data.files.quote.character = c("\""), 
         temporal.coverage = c("1934", "2014"), 
         geographic.description = "University of Michigan Biological Station",
         geographic.coordinates = c("45.558794", "-84.714524", "45.558794", "-84.714524"), #N, E, S, W
         maintenance.description = "ongoing", #completed or ongoing
         user.id = "adrebin",
         package.id = "edi.temp.1")
```

Your EML file will be written to your data directory with the name packageID.xml (ex: edi.188.1.xml). If your EML is valid you will receive the message: EML passed validation!. If validation fails, open the EML file in an XML editor and look for the invalid section. Often a minor tweak to the EML can be made manually to bring it into compliance with the EML schema.  

Your data and metadata form a package that may be uploaded to the EDI data repository. Follow these instructions to upload your data package: https://environmentaldatainitiative.org/resources/assemble-data-and-metadata/step-4-submit-your-data-package/



